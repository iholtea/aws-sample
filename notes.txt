https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/home.html

https://docs.aws.amazon.com/lambda/latest/dg/welcome.html
https://docs.aws.amazon.com/serverless/latest/devguide/serverless-building-apps.html

https://www.srvrlss.io/blog/Amazon-Lambda-docker/

https://www.coursera.org/learn/building-modern-java-applications-on-aws
https://www.coursera.org/learn/building-modern-java-applications-on-aws/home/week/1

https://www.coursera.org/learn/dynamodb-nosql-database-driven-apps
https://www.coursera.org/learn/dynamodb-nosql-database-driven-apps/home/week/1

https://bobbyhadz.com/blog/aws-list-all-resources
Also another good place to start is the billing console

aws lambda invoke --function-name FirstLambda \ 
--cli-binary-format raw-in-base64-out \ 
--payload file://first-lambda-test-02.json out.json

!! See how to create a nice readme.md in github
you can upload images into an img folder and then use them from the readme
https://github.com/awsdocs/aws-doc-sdk-examples/blob/main/javav2/usecases/creating_lambda_apigateway/Readme.md

IAM Roles

A service-linked role is a unique type of IAM role that is linked directly to an AWS service. 
Service-linked roles are predefined by the service and include all the permissions 
that the service requires to call other AWS services on your behalf. 

AWSServiceRoleForSupport
AWSServiceRoleForTrustedAdvisor
    They are Service-Linked Role
    q1: why do they appear in my Roles list in AWS console ?
        I did not create them since they are predefined by AWS, maybe they pop up in that 
        list when the services to which they are linked are used for the first time ?
    q2: I might have screwed up by deleting a Cloud9 service linked role ?
        why did AWS let me delete it if it's predefined by an AWS service :) ?

NOTE
    I think it would be a good practice to tag anything I create in AWS with 
    created-by : iholtea so I know what to delete and what might be stuff created by AWS

https://aws.amazon.com/developer/tools/
    Tools for developing and managing applications on AWS

NOTE
    it would seem one way to enable an aws service to make use of another aws service
    is to use Roles. You attach a role to the first service that grants permissions to 
    access the sencond service. 

 Parameter store
    to use key-value pairs to keep names of aws resources in case they will change
    for example names of the s3 buckets used by an application  

 NoSQL
    in DynamoDB if creating a Books table, we may have Book Title + Author Name as partition key
    but then we want to search also by author name and by book title

    check JavaBrains Cassandra tutorial here: https://www.youtube.com/watch?v=106jIBE9XSc
    We might have a BookById table, and BookByAuthor table 
    Having just book title as partition key is not ideal, as multiple books may have the same title
    (written by different authors)
    Note that we may have multiple entries with the same partition key, it is expected
    so an author name/id can be PK in the BooksByAuthor table because we want to display 
    all the books written be an author when displaying authord details

    but having book title + author name as partition key , i don't know if we could have 
    two additional "indexes" both for book title and for author name to search by both  

 Security related :

    Resource policy
        is attached to a resource like an S3 bucket and specifies what actions are allowed 
        on that resource

    Identity policy
        specifies what services/resources that identity can access.
        ( probably it's not enough for example to attach a role to lambda that it can access S3 buckets
        if the specific bucket does not have a policy to allow actions to be executed)

    Also, the thing with identity policy is that we may add them only for services in our account.
    If we need to allow other account to access different resources we need resource policies

    Bucket policies (check https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)

    Deny access exception some IP address class
	213.233.104.138
	
    {
      "Version": "2012-10-17",
      "Id": "BlockIPAddr",
      "Statement": [
        {
          "Sid": "IPAllow",
          "Effect": "Deny",
          "Principal": "*",
          "Action": "s3:*",
          "Resource": "arn:aws:s3:::test-usage-iholtea/*",
          "Condition": {
            "NotIpAddress": {
                "aws:SourceIp": "1.3.3.5/32"
            }        
          }
        }
      ]
    }

    Create a static website with S3

        step 1: create a bucket with the chosen name , uncheck Block Public Access 
            leave the rest default and hit Create Bucket

        step 2: 
            click on the bucket -> properties tab
            scroll to the bottom the the Static website hosting section and hit Edit   
            check enable
            provide the index and error document html file names
              for example index.html and error.html
            click on Save changes button
        
            At this point if we access the linked provided by AWS for the site like
            http://test-static-site-iholtea.s3-website-us-east-1.amazonaws.com 
            we get 403 Forbidden  Code: AccessDenied
            By default S3 is private and since we did not set any permission to 
            access the objects from this bucket
            NOTE - now we are accessing it from the internet, so as an anonymous
            or un-authenticated user( as opposed when accessing it from a a java client
            using AWS API and having the Admin credentials set up)

        step 3:
            add a bucket policy to enable access
            go to Permission tab in the bucket and edit the Bucket policy

			{
			    "Version": "2012-10-17",
			    "Statement": [
			        {
			            "Sid": "PublicRead",
			            "Effect": "Allow",
			            "Principal": "*",
			            "Action": "s3:GetObject",
			            "Resource": "arn:aws:s3:::test-static-site-iholtea/*"
			        }
			    ]
			}

API Gateway - there are multiple types
    Websocket
        statefull and bi-directional communication. Used for real-time apps like chats.
    Http 
        used to proxy back-end resources and are supposed to be simple and fast.
        supports authentication, can call lambda functions
        so it takes a request, authorises it and passes to the backend implementation(lambda, other http endpoint)
    REST
        similar with Http but has additional functionality.
        gives control over the request and response, validation, transformation of the data payload,
        may provide mocked responses.

Create a new API.
    go to API Gateway, choose REST API and click Build
    select New API type
    give it a name: todos-manual 

Create a Resource - this is like and endpoint to which we then can add methods.
    click Create Resource 
    give it a name like hello. The path will hava the same name.
    now we have the hello resource and by default it has the http OPTIONS method

Create a Method for a Resource
    
    select type - GET/POST/DELETE etc
    select integration type - Lambda, Mock, Http endpoint     
    
    after creation we can control what happens during the request/response flow
    Method Request settings 
        apply authorization, validate data payload
    Integration request 
        specify integration type like Mock, Lambda
        apply data transformation on payload if necessary
    Integration response 
        this is an http response encapsulating the backend response    
        so we cand configure how backed response maps to http response
        we can apply data transformation
    Method response settings
        can apply validations on the response data
        specify how to send different http status codes and associated headers/payload 

    Test tab where we could add query strings and headers and test the current method

    Models: 
        validate request/responses with models
        defines the structure of the request
        are created using JSON schemas
        if the request payload does not match the schema API gateway will return a 
            400 error http response. So no need to call lamba/backend for this taskß 
    Mappings: transform request/responses with mappings 
        might be needed when backend expects the data in a format different
        from the one the client sends.  
        mapping are written in VTL - velocity template language 

To create an integration response for a Mock type method
    Click Edit on the response. There is a default one for 200 status code
    Expand Mapping templates and add the json body we want to return as mock
    Ex for a HelloWorld:
    {
      "message": "hello, world!"
    }

    Use VTL instead of just plain JSON to create a dynamic response 
    depending on query string parameters
    see https://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html
    
    #set($name=$input.params('name'))
    #set($lang=$input.params('lang'))
    #if( $lang == 'ro' )
      {
        "message": "Salut, $name"
      }
    #elseif( $lang == 'es' )
      {
        "message": "Hola, $name"
      }
    #else
      {
        "message": "Hello, $name"
      }
    #end    

See https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/java_s3_code_examples.html#s3_GetObject_java_topic
Section 


https://www.reddit.com/r/vscode/comments/su4ebg/how_to_set_tab_spacing_per_file_type_for_all/
section Get started with buckets and objects

REST Spring Boot and CORS : https://www.baeldung.com/spring-cors

!!!! Some idea
	use the AWS parameter store to indicate if the TODO-s app storage should be JSON file on S3 or Dynamo DB
	use some factories inside the lambdas to determine that :)
	
To create an API Gateway resource with Path Parameters :
	1.  create the "main" resource, for example todos
	2.	select the todos resource and click create resource - this will enable to create a sub-resource
		in the name of the resource provide the path parameter ex: {todoId}

X-Ray vs Cloud Watch as observability mechanisms.
	
	CloudWatch is a common place where we can see the logs of different AWS resources.
	
	For example for a Lambda Function we can do logging using 
	LambdaLogger logger = context.getLogger();
	logger.log("some message")
	(System.out.println may or may not have the same outcome)
	
	Since multiple instances of a lambda function may run at the same time, having 
	a unique log file to check would not be a trivial task.
	
	So AWS provides the CloudWatch. When we to a LabdaLogger.log() in fact we are doing
	a call to some AWS API which will write to CloudWatch logs
	
	CloudWatch has Log Groups - so for a MyLambda lambda function there will be a group
	Log Stream - each instance of the MyLambda function will be a different entry/file in the group.
	
	Note that for example for Lambda Function it's not necessary for us to create a log group.
	AWS seems to do it automatically the first time the function runs.
	
	  
		
Lambda-s and permissions.
	
	Execution role: this specifies what services your lambda can access	
		for example - access DynamoDB, S3, write logs to CloudWatch, add tracing to X-Ray
		
		When creating a LambdaFunction from the Web UI if we do not specify an IAM role 
		as execution role, AWS will create one automatically. It has CloudWatch permissions:
		FirstLambda-role-4vl4e1cp has one policy
			AWSLambdaBasicExecutionRole-6333557d-55de-40b2-9cbd-f6ae76b76339
			{
			    "Version": "2012-10-17",
			    "Statement": [
			        {
			            "Effect": "Allow",
			            "Action": "logs:CreateLogGroup",
			            "Resource": "arn:aws:logs:us-east-1:285469546962:*"
			        },
			        {
			            "Effect": "Allow",
			            "Action": [
			                "logs:CreateLogStream",
			                "logs:PutLogEvents"
			            ],
			            "Resource": [
			                "arn:aws:logs:us-east-1:285469546962:log-group:/aws/lambda/FirstLambda:*"
			            ]
			        }
			    ]
			}
		
		We should probably create a specific Role with the needed permission like
		CloudWatch, DynamoDb access. We should be able to select those from the 
		available Policies provided by AWS 			
		
	
	Resource based policy: this specifies who can access this resource
		for example - for a certain lambda function to be invoked by an API Gateway
		endpoint a policy has to be added to the lambda to permit this.
		
		Note that when binding a Lambda Function to an API Gateway resource/method
		using Web UI , AWS will automatically create the necessary IAM policies for
		the lambda to be called.
		
		For a /todo GET request it creates:
		{
		  "ArnLike": {
		    "AWS:SourceArn": "arn:aws:execute-api:us-east-1:285469546962:r4cwbi98f9/*/GET/todos"
		  }
		} 	
		
		For a /todo/{todoID} so different path, it creates:					
		{
		  "ArnLike": {
		    "AWS:SourceArn": "arn:aws:execute-api:us-east-1:285469546962:r4cwbi98f9/*/GET/todos/*"
		  }
		}
		
		Q:  note that above r4cwbi98f9 is the id of the API Gateway 
			could these be re-united under a single policy to allow all request from
			that specific API Gateway, so all http methods like GET, POST, DELETE, etc
			and all paths ? Although we do not have so many :) 
			
		
////////////////

Create all TODOs related resources for the Web UI version using Manual-Todo prefix
Create all TODOs related resource for the AWS SDK provisioning using SDK-TODO prefix
use iholtea suffix where global unique names are required, like S3 buckets for example

-   Create an API Gateway of type REST API with name manual-todo

-   Creat a resource /footer , check enable CORS. 
	This will automatically add the OPTIONS Http method. 
	Choose Mock as integration type
	Provide response integration type as mock
	in order to be used to display some custom info as the footer of the page.
	Gets 2 query string parameters - lang( for language) and company ( for company name )

-   Deploy the API and link it to a stage
	
	Although without a stage it has a public access link like
	https://95k6wo08hg.execute-api.us-east-1.amazonaws.com
	it cannot be accessed outside AWS console, it will display an Forbidden message
	
	So we need a stage - call it test
	It seems we cannot create a stage without a deployment(? to be confirmed)
	So hit deploy and select *New Stage*, give it a stage name: test
	
	This provides the Invoke URL: https://95k6wo08hg.execute-api.us-east-1.amazonaws.com/test
	Invoke with params: https://95k6wo08hg.execute-api.us-east-1.amazonaws.com/test/footer?lang=ro&company=Apple
	
	We may also set throttling parameters, logs and tracing, etc 
	
-   Create IAM role for execution of lambda
	( specifies what service a lambda function can access )	
	
	Name: Manual-Todo-Lambda-Role
	Attached policies:
		CloudWatchLogsFullAccess
		AmazonDynamoDBFullAccess
		
-	Create LambdaFunction with name ManualTodoLambda
	choose as execution role the one created above instead of letting the UI create a new one.
	
-	Go to the manual-todo API Gateway and create the /todos and /todos/{todoId} resources
	check enable CORS for both
	To create the sub-resource with path parameter /todos/{todoId} hit create a resource
	after selecting the /todos resource - this will create it as a "child" resource
	provide {todoId} as name
	
	Check the Lambda proxy integration
	Choose lambda function as integration type and select the ManualTodoLamba
	from the drop-down. It will appear by its ARN, since we did not publish the Lambda
	Lambda-s can be published, they may have multiple versions.
	We can create ALIAS-es to point to some version( specific one or $LATEST)
	In order to support multiple environments like TEST, PROD, etc
	
	Since we are using just one environment, it's not mandatory to publish the lambda
	API Gateway will reference it by its ARN and will always invoke the $LATEST uploded code. 		
		
!!!!
	https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway-tutorial.html
	https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html
	or 
	headers: {
      "Access-Control-Allow-Origin": "*", // Required for CORS support to work
      "Access-Control-Allow-Credentials": true, // Required for cookies, authorization headers with HTTPS
    },
	
	For the the execution role of lambda, to allow access to CloudWatch and DynamoDB
	instead of using those build-in AWS policies containing full access to the service
	we could create a custom policy
	
	1.  Open the Policies page of the IAM console.
	2.  Choose Create Policy.
	3.  Choose the JSON tab, and then paste the following custom policy into the JSON editor.
	
	{
	  "Version": "2012-10-17",
	  "Statement": [
	    {
	      "Sid": "Stmt1428341300017",
	      "Action": [
	        "dynamodb:DeleteItem",
	        "dynamodb:GetItem",
	        "dynamodb:PutItem",
	        "dynamodb:Query",
	        "dynamodb:Scan",
	        "dynamodb:UpdateItem"
	      ],
	      "Effect": "Allow",
	      "Resource": "*"
	    },
	    {
	      "Sid": "",
	      "Resource": "*",
	      "Action": [
	        "logs:CreateLogGroup",
	        "logs:CreateLogStream",
	        "logs:PutLogEvents"
	      ],
	      "Effect": "Allow"
	    }
	  ]
	}
	
	4.  Choose Next: Tags.
	5.  Choose Next: Review.
	6.  Under Review policy, for the policy Name, enter lambda-apigateway-policy.
	7.  Choose Create policy.
	
	Create the role:
	1.  Open the Roles page of the IAM console.
	2.  Choose Create role.
	3.  For the type of trusted entity, choose AWS service, then for the use case, choose Lambda.
	4.  Choose Next.
	5.  In the policy search box, enter lambda-apigateway-policy.
	6.  In the search results, select the policy that you created (lambda-apigateway-policy), 
		and then choose Next.
	7.  Under Role details, for the Role name, enter lambda-apigateway-role, then choose Create role.

	Later in the tutorial, you need the Amazon Resource Name (ARN) of the role you just created. 
	On the Roles page of the IAM console, choose the name of your role (lambda-apigateway-role) 
	and copy the Role ARN displayed on the	
	( I think the ARN role is needed if we create the lambda from CLI to provide the execution
	role. In the Web UI i think you can find it in the drop-down )
	
	https://fontawesome.com/icons/trash-can?f=sharp&s=regular
	
Add Usage Plan / API key to limit number of requests
	
	1.	Go To API Gateway section and then Usage Plans
		Create Usage plan
			provide name: manual-todo-usage-lan
			Throttling
				Rate - requests per second: 5
				Burst - concurrent requests: 5
				Quota - requests per month / week / day:  100 per day  
	
	2.	Add API stage ( stage of an API Gateway ) to the usage plan		
		it seems it's not limiting requests just by adding the stage to the plan
		we need to add also API keys
		
	3.	Create API Key from API Gateway section
		name: manual-todo-api-key	
		leave auto generate type 
		add API key to usage plan ( can be done both from the API key or from Usage plan )
		
	4.	We need to go to our API in API Gateway and for each method of each endpoint
		go to method request tab and enable API Key required
		
	5.	all requests to API gateway not need to provide the http header
		x-api-key  with the API Key value: AGdASLxD8t3chg1F6uCe251G2vFWrdQt6ZgUnP8K	
		
	Note:
		it would seem any change to Usage Plan require a couple of seconds or a minute
		to take effect
		
			
		
		
	
HTTP status:
	200 Success
	201 Created - to use after a POST request that creates some item
	204 No Content - may be used after a DELETE request that successfully removes data
	401 Unauthorized - may be used to let the client know it has no credential to access a resource
	400 Bad Request - like a validation error 
	403 Forbidden - for example API Gateway when required API key not provided	
	404 Not Found
	429 Too Many Request - for example API Gateway when request limit exceeded
	500 Server Error 
	
///////////////////////////////////////////
public static void main(String[] args) throws Exception {
    AmazonDynamoDB dynamodb = null;
    try {
        // Create an in-memory and in-process instance of DynamoDB Local that skips HTTP
        dynamodb = DynamoDBEmbedded.create().amazonDynamoDB();
        // use the DynamoDB API with DynamoDBEmbedded
        listTables(dynamodb.listTables(), "DynamoDB Embedded");
    } finally {
        // Shutdown the thread pools in DynamoDB Local / Embedded
        if(dynamodb != null) {
            dynamodb.shutdown();
        }
    }
    
    // Create an in-memory and in-process instance of DynamoDB Local that runs over HTTP
    final String[] localArgs = { "-inMemory" };
    DynamoDBProxyServer server = null;
    try {
        server = ServerRunner.createServerFromCommandLineArgs(localArgs);
        server.start();

        dynamodb = AmazonDynamoDBClientBuilder.standard().withEndpointConfiguration(
            // we can use any region here
            new AwsClientBuilder.EndpointConfiguration("http://localhost:8000", "us-west-2"))
            .build();

        // use the DynamoDB API over HTTP
        listTables(dynamodb.listTables(), "DynamoDB Local over HTTP");
    } finally {
        // Stop the DynamoDB Local endpoint
        if(server != null) {
            server.stop();
        }
    }
}

https://github.com/aws/aws-sdk-java-v2/blob/93269d4c0416d0f72e086774265847d6af0d54ec/services-custom/dynamodb-enhanced/src/test/java/software/amazon/awssdk/extensions/dynamodb/mappingclient/functionaltests/TransactGetItemsTest.java
/////////////////////////////


	
TODOs in UI
	
	0.	Add http status code error handling in client JS
		403 / 429 related api key / too many request
		Later add validation of accessing data, set 400 bad request
		if user tries to access data that's not his
		( too expensive to determine if that data is on some other user
		or does not exist )
		Use HttpStatusCode as numeric as the first parameter of the callback methods
		use a 999 value for the Generic error, like maybe API Gateway not reachable
		
		create a div at the bottom of the screen to display error messages
		each callback should delete the error text - should be easier
		

	1.  add order both to items in a todo list and for the todo list
		
		For Todo list we could use a timestamp ( for now its just a yyyy-mm-dd )
		
		As items are added as a batch request, we should use a column to order them
		
		Q: what is the "natural order" for PK/SK in Dynamo ? \
		A: lexicographical order for string, numeric for numbers
		
		For items add a ItemOrder column, we might want to change it from UI
		also for items have a check to oder items also by done/un-done
		
		NOTE:
			A time-based UUID, also known as the version 1 UUID, is generated using 
			the current time and a unique identifier specific to the computer or network 
			that produces the UUID. The timestamp ensures the UUID is unique, 
			even if multiple UUIDs are generated simultaneously.

			We’ll find two new versions of the standard (v6 and v7) that are time-related 
			in the libraries implemented below.

			Version 1 presents several advantages – the time sorted id is fitter to be a primary
			key in a table, and containing the creation timestamp can help with analysis and debugging. 
			It also has some disadvantages – the chance of collision is slightly higher when generating multiple IDs from the same host. 
			We’ll see if this is an issue later on.
			
			Also, i think it's useful for relational DBs due to the algorithm they use to create
			the ordered index. A complete random Key means a costly insert as the index trees
			need to be rebalanced quite a lot as oposed to an ordered key like a sequence
			
		https://www.baeldung.com/java-generating-time-based-uuids	
		
		<dependency>
		    <groupId>com.fasterxml.uuid</groupId>
		    <artifactId>java-uuid-generator</artifactId>
		    <version>4.1.0</version>
		</dependency>
		
		System.out.println("UUID Version 1: " + Generators.timeBasedGenerator().generate());
		System.out.println("UUID Version 6: " + Generators.timeBasedReorderedGenerator().generate());
		System.out.println("UUID Version 7: " + Generators.timeBasedEpochGenerator().generate());
		
		Single Table idea
		Partition key:  userEmail
			as DynamoDb is using a hash function to determine the "bucket" where the records
			will be written, it would be preferable that the PK would be uniformly distributed
			So and UUID would be better than an email or name ( more random )
			But the user logs with the email how to get the UUID ?
			( would having a separate table with just email and uuid and email PK worth it 
			to do once per login query ?)
			Then the email will be part of the JSON token sent with every request ?
		Sort Key
			L/I is its a list or item to be able to get just the lists
			#list-uuid
			#item_uuid
			For a list:   SK:  L#list-uuid   // query Lists by starts_with l#
			For an item:  SK:  I#list-uuid#item-uuid
			
			But if a list record starts with L# and an Item record startd with I# we cannot
			select them both in a single query, as SK condition supports just begins_with, it 
			does not support NOT or other conditions.
			
			If we do not have the L# / I# prefix, and we have 
			SK: list_uuid for lists
			SK  list_uuid#item_uuid 
			then we can select them both in the same query, but it will not be efficient to select
			just the lists without the items, as we'll need to select begins_with( list_uuid ) and then
			filter out the items - by contains("#") for example, but filtering happens after all the 
			records were selected and the read capacity consumed.
			So in the end it depends on how we plan to implement the front-end.
			If we want to select just lists without items or not
		
		Another thing to keep in mind is that you pay for DynamoDB by RCU ( read units )
		So even if record is small ( 1 RCU = up to KB ) we still pay 1 or 0.5 RCU per read.
		So it might be more cost effective to have duplicate data in records in a table
		than reading smaller data from 2 different tables of from 2 different queries.	
		On the other hand each update of a record means it is first read by its PK(+SK)
		attributes updated and then written back - to keep in mind for frequent updates or records
		
		!!! NOTE:
		Key condition expressions for the Query operation
			for PK support just equals operator: pk = :p
			for SK it support a=b, a<b, a<=b, a>b, a=>b, a BETWEEN b AND c, begins_with( a, substr)
				so no other conditions, especially not OR condition
		We need Filter Expression to use OR, CONTAINS, EXISTS and other functions.	 
		But:
		A filter expression is applied after a Query finishes, but before the results are returned. 
		Therefore, a Query consumes the same amount of read capacity, regardless of whether a filter expression is present.
		
		Pagination in DynamoDb:
		https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.Pagination.html
		
				   
	2.	implement authorization for delete / update operations
		As the user email is server side an PK for TodoList-s, the operations will not work if a bad actor
		is trying to delete TodoList-s using random Uuids
		If we keep the TodoItem with PK List Uuid, then we need to check if the List UUID belongs to the
		logged in user
		( this might be a pro to use only user email as PK and a hierarchical SK )
		
		
	3.	each operation on a TodoList, like delete/check/update/add new item should modify
		the TodoList lastUpdate field which should be used to order the lists	
		
		i think we could use a local secondary index being the timestamp of last update
		( but is this a good idea, to update so often the field that is an index ? )
		Maybe it's a better idea to just scan within for the users all the todo lists
		
		In fact we should set the lastUpdate when a user clicks to see the details of a todoList
		to minimize operations, especially if the timestamp would be a LSI ( local secondary index )
		update it on the server only if the lastUpdate is not the same day as in the database
		so we do not update it more times per day/session
		
		https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html
		( it seems the timestamp stuff is a valid thing :) )
		It seems we do not have an order by. So we could select first 10 newer than 1 year
		If nothing found, select first 10 from whenever
		
		AWS docs: 
			When you add or update a table item, DynamoDB updates all local secondary indexes that are affected. 
		If the indexed attributes are defined in the table, the local secondary indexes grow too. 
			In a DynamoDB table, the combined partition key value and sort key value for each item must be unique. 
		However, in a local secondary index, the sort key value does not need to be unique for a given partition key value. 
		If there are multiple items in the local secondary index that have the same sort key value, 
		a Query operation returns all of the items that have the same partition key value. 
		In the response, the matching items are not returned in any particular order.
		
		
	
	3.  handle DynamoDbException in Repository implementation. 
		Throw custom runtime exceptions or catch them in business layer and create appropriate response to client 
		
	4.  Check the "responsive-ness" of the design, too much space to the right on smaller resolutions	
		
	4.	add USage plan with API key - check internets
		https://www.youtube.com/watch?v=AxhbAx4PTow	
		Note - we can do some throttling on the API Gateway stage
			but we can provide only max number of requests pe second and concurrent requests
			With API Key usage plans we can limit number of requests per month also
		
	5.  add some pagination to either lists or items or both	
	
	9.	Lower priority as when fetching the list of Todos we might not fetch all the List attributes
		( if they might contain lots of data ):
		when fetching a TodoList by ID, we should just fetch its Items
		as the TodoList details are already stored in the globalData.todos
	
		
https://www.discworldemporium.com/reading-order/

Fa programare la GTT motors pt ITP si revizie

https://johntipper.org/setting-up-the-aws-cdk-with-java-and-gradle/
https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/setup-project-gradle.html

API Gateway and CORS :)
https://github.com/aws-samples/aws-cdk-examples/blob/main/java/api-cors-lambda-crud-dynamodb/cdk/src/main/java/software/amazon/awscdk/examples/CorsLambdaCrudDynamodbStack.java

https://stackoverflow.com/questions/62449187/deploy-multiple-api-gateway-stages-with-aws-cdk

TODO
	one difference is for the DynamoDbTable
	the manual one has provisioned 1 Read/Write Capacity Units while the CDK one seems
	to have 5 which is the default. Does it makes sense to modify
	
	for CORS - since it seems i did not do special stuff for the manual one
	the manual thing had the default OPTIONS method for each endpoint
	with whatever the online AWS console creates
	
	maybe creating an options method from CDK does the same thing ??
	to be tried		
	
	So it seems in the first instance a GET request to /todos to obtain the list of
	todos does not work and it's related to an OPTIONS request
	
	Probably the browser does first an OPTIONS request for the GET reqeust
	because it's not a "simple" GET request, we are sending also the x-api-key
	so it's using OPTIONS to ask the server if it accepts that kind of header
	
The OPTIONS method config for a API resource created with aws web console:
	
	Integration response:
		Status code: 200
		Content handling: passtrhough
		default mapping: true
		Header mappings		
			method.response.header.Access-Control-Allow-Headers	'Content-Type,Authorization,X-Amz-Date,X-Api-Key,X-Amz-Security-Token'
			method.response.header.Access-Control-Allow-Methods	'DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT'
			method.response.header.Access-Control-Allow-Origin	'*'
	Method response:
		Response headers
			Access-Control-Allow-Headers
			Access-Control-Allow-Methods
			Access-Control-Allow-Origin	
		Response body
			content type
				application/json	
				
/////////////

https://sabe.io/blog/javascript-status-code-fetch-http-response?expand_article=1
https://4geeks.com/lesson/the-fetch-javascript-api

/////////////

JS async.

When we have a piece of code that does asynchronous work like XMLHttpRequest
or newer build-in fetch to get data from some external API or a setTimeout() call
to simulate that we are waiting for some data before retrieved ->
we CANNOT directly return that data from the function, as the runtime cannot know
when the data will be available, all it can do is to register some piece of code
to be run when data is available.

Example : use setTimeout() to simulate an API call to get user email

const users = new Map();
users.set('gigi#aa', { email: 'gigi@yahoo.com', fullName: 'Gigi Gigescu' });
users.set('fifi#bb', { email: 'fifi@gmail.com', fullName: 'Fifi Fifescu'});

function getUserEmail( userId, pass ) {
  setTimeout( () => {
    return users.get( userId + '#' + pass );
  }, 1500 );
}

console.log('Start');

const user = getUserEmail( 'fifi', 'bb' );
console.log(user);

console.log('Finish');

Running the code above will result in user variable being undefined
The getUserEmail() does not and cannot return the data it retrieves.
It can register a callback function to be run when data is available.
For example:

function getUserEmail( userId, pass, callback ) {
  setTimeout( () => {
    callback( users.get( userId + '#' + pass ) );
  }, 1500 );
}

// process data when available
getUserEmail( 'fifi', 'bb', (user) => {
  console.log( `Retrived user: ${user.email} name: ${user.fullName}`);
});  


Callback hell.

If we need to make multiple API calls that depend on one another we will
need to chain those callbacks for each call.

So first we need to get the userEmail using userId and password
Then after we have the userEmail we need to get related videos

const videos = new Map();

videos.set( 'gigi@yahoo.com',  
  [ { videoId: 'video-01', title: 'Video 01' } ] );

videos.set( 'fifi@gmail.com', [ 
    { videoId: 'video-02', title: 'Video 02' },
    { videoId: 'video-03', title: 'Video 03' } ] );
    
function getVideosByEmail( email, callback ) {
  setTimeout( () => {
    callback( videos.get( email ) );
  }, 800 );
} 

And to process the data:   

getUserEmail( 'fifi', 'bb', (user) => {
  console.log( `Retrived user: ${user.email} name: ${user.fullName}`);
  getVideosByEmail(user.email, (videos) => {
    console.log(`Retrived first video: ${videos[0].title}`);
  }); 
});

If for example for a video we would make another call to get video details information
we would need to add another callback to process the video details

The code can be made to look a bit nicer by extracting the inline defined callbacks
into external functions

getUserEmail('fifi', 'bb', getUserCallback);

function getUserCallback(user) {
  console.log( `Retrived user: ${user.email} name: ${user.fullName}`);
  getVideosByEmail(user.email, getVideosCallback);
}

function getVideosCallback(videos) {
  console.log(`Retrived first video: ${videos[0].title}`);  
}


///////////

const xhr = new XMLHttpRequest();
xhr.open('GET', baseUrl, true);
xhr.send();

fetch("https://jsonplaceholder.typicode.com/todos/1");

Handling error for fetch
https://dev.to/dionarodrigues/fetch-api-do-you-really-know-how-to-handle-errors-2gj0

try {
  const response = await fetch('https://restcountries.com/v4.1/all');

  if (response.ok) {
    console.log('Promise resolved and HTTP status is successful');
    // ...do something with the response
  } else {
    // Custom message for failed HTTP codes
    if (response.status === 404) throw new Error('404, Not found');
    if (response.status === 500) throw new Error('500, internal server error');
    // For any other server error
    throw new Error(response.status);
  }
} catch (error) {
  console.error('Fetch', error);
  // Output e.g.: "Fetch Error: 404, Not found"
}						


To test smth like

async function f1(param1) {
	setTimeout( () => {
		console.log(param1)
		return 'f1-data';
	}, 10000);
}

async function f2(param2) {
	setTimeout( () => {
		console.log(param2);
		return 'f2-data ' + param2; 
	}, 5000);
}

async function getResults() {
	const resp1 = await f1('some-data');
	const resp2 = await f2(resp1);
	console.log(resp2);
}

getResults();

        
